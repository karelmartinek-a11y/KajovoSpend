diff --git a/.gitignore b/.gitignore
--- a/.gitignore
+++ b/.gitignore
@@ -1,19 +1,22 @@
 # Virtualenvs
 .venv/
 
 # Python artifacts
 __pycache__/
 *.pyc
 *.pyo
 *.pyd
 *.log
 .env
 .env.local
 
 # App runtime data
 OUTPUT/
 INPUT/
 service.pid
+var/
 
 # IDE
 .idea/
 .vscode/
diff --git a/src/kajovospend/service/processor.py b/src/kajovospend/service/processor.py
--- a/src/kajovospend/service/processor.py
+++ b/src/kajovospend/service/processor.py
@@ -1,26 +1,27 @@
 from __future__ import annotations
 
 import datetime as dt
 import shutil
 import io
 import re
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple, List
 
 from PIL import Image
 from pypdf import PdfReader
 from sqlalchemy import text
 
 from kajovospend.db.models import ImportJob
 from kajovospend.db.queries import (
     add_document,
     create_file_record,
     rebuild_fts_for_document,
     upsert_supplier,
 )
 from kajovospend.extract.parser import extract_from_text, postprocess_items_for_db
 from kajovospend.integrations.ares import fetch_by_ico, normalize_ico
 from kajovospend.ocr.pdf_render import render_pdf_to_images
 from kajovospend.ocr.rapidocr_engine import RapidOcrEngine
 from kajovospend.utils.hashing import sha256_file
+from kajovospend.utils.text_quality import compute_text_quality, summarize_text_quality
 
 
 def safe_move(src: Path, dst_dir: Path, target_name: str) -> Path:
@@ -46,52 +47,95 @@ class Processor:
         except Exception as e:
             self.log.warning(f"OCR init failed; will quarantine documents. Error: {e}")
             self.ocr_engine = None
 
-    def _ocr_pdf_pages(self, pdf_path: Path, status_cb=None) -> Tuple[List[str], List[float], int]:
+    def _ocr_pdf_pages(self, pdf_path: Path, status_cb=None) -> Tuple[List[str], List[float], int, str, Dict[str, Any]]:
         """
         Vrátí OCR text po jednotlivých stránkách:
         - embedded text: per page extract_text()
         - jinak: render->OCR per page
         """
+        debug: Dict[str, Any] = {"path": str(pdf_path)}
+
         # Try embedded text first (page-by-page)
         if status_cb:
             status_cb("PDF: čtu text (bez OCR)…")
+        texts: List[str] = []
         try:
             reader = PdfReader(str(pdf_path))
-            texts: List[str] = []
             for page in reader.pages:
                 t = page.extract_text() or ""
                 texts.append(t)
             # if at least one page has real text, treat as embedded text mode
             if any((t or "").strip() for t in texts):
                 confs = [0.95 if (t or "").strip() else 0.0 for t in texts]
-                return texts, confs, len(texts)
+                emb_summary = summarize_text_quality([compute_text_quality(t) for t in texts])
+                pages_with_text = sum(1 for t in texts if (t or "").strip())
+                debug["embedded"] = emb_summary
+                debug["embedded_pages_with_text"] = pages_with_text
+                self.log.info(
+                    "PDF text source: embedded pages_with_text=%s/%s quality=%s",
+                    pages_with_text,
+                    len(texts),
+                    emb_summary,
+                )
+                return texts, confs, len(texts), "embedded", debug
         except Exception:
-            pass
+            texts = []
+
+        emb_summary = summarize_text_quality([compute_text_quality(t) for t in texts])
+        debug["embedded"] = emb_summary
+        debug["embedded_pages_with_text"] = 0
+
+        # fallback to image OCR
+        if self.ocr_engine is None:
+            debug["reason"] = "ocr_engine_missing"
+            self.log.info(
+                "PDF text source: ocr skipped reason=ocr_engine_missing embedded_quality=%s",
+                emb_summary,
+            )
+            return [], [], 0, "none", debug
 
-        # fallback to image OCR
-        if self.ocr_engine is None:
-            return [], [], 0
         dpi_cfg = int(self.cfg["ocr"].get("pdf_dpi", 200))
         dpi = max(300, dpi_cfg)
         if status_cb:
             status_cb(f"PDF: render na obrázky ({dpi} DPI)…")
         images = render_pdf_to_images(pdf_path, dpi=dpi)
         texts2: List[str] = []
         confs2: List[float] = []
         for idx_page, img in enumerate(images, start=1):
             if status_cb:
                 status_cb(f"OCR: strana {idx_page}/{len(images)}…")
             t, c = self.ocr_engine.image_to_text(img)
             texts2.append(t or "")
             confs2.append(float(c or 0.0))
-        return texts2, confs2, len(images)
+
+        ocr_summary = summarize_text_quality([compute_text_quality(t) for t in texts2])
+        debug["ocr"] = ocr_summary
+        debug["dpi"] = dpi
+        debug["ocr_conf_avg"] = float(sum(confs2) / len(confs2)) if confs2 else 0.0
+        debug["ocr_conf_min"] = float(min(confs2)) if confs2 else 0.0
+        debug["ocr_conf_max"] = float(max(confs2)) if confs2 else 0.0
+
+        self.log.info(
+            "PDF text source: ocr reason=embedded_empty dpi=%s pages=%s conf_avg=%.3f conf_min=%.3f conf_max=%.3f quality=%s",
+            dpi,
+            len(images),
+            debug["ocr_conf_avg"],
+            debug["ocr_conf_min"],
+            debug["ocr_conf_max"],
+            ocr_summary,
+        )
+        return texts2, confs2, len(images), "ocr", debug
 
     def _merge_extracted_by_key(self, per_page: List[Tuple[int, Any, str, float]]) -> List[Dict[str, Any]]:
         """
         per_page: [(page_no, Extracted, full_text, ocr_conf), ...]
@@ -142,14 +186,24 @@ class Processor:
     def _ocr_image(self, path: Path, status_cb=None) -> Tuple[str, float, int]:
         if self.ocr_engine is None:
             return "", 0.0, 1
         if status_cb:
             status_cb("OCR: zpracovávám obrázek…")
         with Image.open(path) as img:
             t, c = self.ocr_engine.image_to_text(img)
+        try:
+            q = summarize_text_quality([compute_text_quality(t or "")])
+            self.log.info("Image text source: ocr conf=%.3f quality=%s", float(c or 0.0), q)
+        except Exception:
+            pass
         return t, c, 1
 
     def _guess_supplier_ico_from_text(self, text: str) -> Optional[str]:
         """Best-effort: pokud OCR nevyčetl IČO pomocí kontextu, zkus najít 8místné kandidáty.
 
@@ -219,7 +273,7 @@ class Processor:
         if existing:
             # duplicate file, move to DUPLICITY
             out_base = Path(self.cfg["paths"]["output_dir"])
             dup_dir = out_base / self.cfg["paths"].get("duplicate_dir_name", "DUPLICITY")
             moved = safe_move(path, dup_dir, path.name)
-            return {"status": "DUPLICATE", "sha256": sha, "moved_to": str(moved)}
+            return {"status": "DUPLICATE", "sha256": sha, "moved_to": str(moved), "text_method": None, "text_debug": {}}
 
         if status_cb:
             status_cb("Začínám vytěžování…")
 
         # OCR
         min_conf = float(self.cfg["ocr"].get("min_confidence", 0.65))
         pages = 1
         per_doc_chunks: List[Dict[str, Any]] = []
+        text_method: Optional[str] = None
+        text_debug: Dict[str, Any] = {}
 
         if path.suffix.lower() == ".pdf":
-            page_texts, page_confs, pages = self._ocr_pdf_pages(path, status_cb=status_cb)
+            page_texts, page_confs, pages, text_method, text_debug = self._ocr_pdf_pages(path, status_cb=status_cb)
             if not page_texts:
                 # no text => hard quarantine later
                 page_texts = []
                 page_confs = []
                 pages = 0
             per_page: List[Tuple[int, Any, str, float]] = []
             for i, t in enumerate(page_texts, start=1):
                 ex = extract_from_text(t or "")
                 per_page.append((i, ex, t or "", float(page_confs[i - 1] if i - 1 < len(page_confs) else 0.0)))
             # Merge multi-page invoices deterministically by key
             per_doc_chunks = self._merge_extracted_by_key(per_page)
         else:
             ocr_text, ocr_conf, pages = self._ocr_image(path, status_cb=status_cb)
+            text_method = "image_ocr"
+            try:
+                text_debug = {"ocr": summarize_text_quality([compute_text_quality(ocr_text or "")]), "ocr_conf": float(ocr_conf or 0.0)}
+            except Exception:
+                text_debug = {"ocr_conf": float(ocr_conf or 0.0)}
             ex = extract_from_text(ocr_text or "")
             per_doc_chunks = [{
                 "page_from": 1,
                 "page_to": 1,
                 "extracted": ex,
                 "full_text": ocr_text or "",
                 "ocr_conf": float(ocr_conf or 0.0),
                 "key": None,
             }]
@@ -380,6 +436,8 @@ class Processor:
                         return {
                             "status": "DUPLICATE",
                             "sha256": sha,
                             "file_id": file_record.id,
                             "moved_to": str(moved),
                             "duplicate_of_document_id": int(dup[0]),
+                            "text_method": text_method,
+                            "text_debug": text_debug,
                         }
                 except Exception as e:
                     reasons.append(f"dup-check selhal: {e}")
                     requires_review = True
@@ -440,10 +498,14 @@ class Processor:
 
         return {
             "status": status,
             "sha256": sha,
             "file_id": file_record.id,
             "document_ids": created_doc_ids,
             "moved_to": str(moved),
+            "text_method": text_method,
+            "text_debug": text_debug,
         }
diff --git a/src/kajovospend/utils/text_quality.py b/src/kajovospend/utils/text_quality.py
new file mode 100644
--- /dev/null
+++ b/src/kajovospend/utils/text_quality.py
@@ -0,0 +1,118 @@
+from __future__ import annotations
+
+from typing import Any, Dict, List
+
+
+def compute_text_quality(text: str) -> Dict[str, Any]:
+    t = text or ""
+    total = len(t)
+    non_ws = sum(1 for ch in t if not ch.isspace())
+    printable = sum(1 for ch in t if ch.isprintable())
+    letters = sum(1 for ch in t if ch.isalpha())
+    digits = sum(1 for ch in t if ch.isdigit())
+    repl = t.count("\ufffd")
+    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
+    avg_line_len = (sum(len(ln) for ln in lines) / len(lines)) if lines else 0.0
+    unique_ratio = (len(set(t)) / total) if total else 0.0
+
+    def _r(num: int, den: int) -> float:
+        return float(num) / float(den) if den else 0.0
+
+    return {
+        "chars_total": int(total),
+        "chars_non_ws": int(non_ws),
+        "chars_printable": int(printable),
+        "chars_letters": int(letters),
+        "chars_digits": int(digits),
+        "replacement_chars": int(repl),
+        "lines_nonempty": int(len(lines)),
+        "avg_line_len": float(avg_line_len),
+        "unique_char_ratio": float(unique_ratio),
+        "ratio_non_ws": _r(non_ws, total),
+        "ratio_printable": _r(printable, total),
+        "ratio_letters": _r(letters, non_ws),
+        "ratio_digits": _r(digits, non_ws),
+        "ratio_replacement": _r(repl, total),
+    }
+
+
+def summarize_text_quality(metrics: List[Dict[str, Any]]) -> Dict[str, Any]:
+    if not metrics:
+        return {
+            "pages": 0,
+            "pages_nonempty": 0,
+            "chars_total": 0,
+            "chars_non_ws": 0,
+            "ratio_printable": 0.0,
+            "ratio_non_ws": 0.0,
+            "ratio_letters": 0.0,
+            "ratio_digits": 0.0,
+            "ratio_replacement": 0.0,
+            "avg_line_len": 0.0,
+        }
+
+    pages = len(metrics)
+    pages_nonempty = sum(1 for m in metrics if int(m.get("chars_non_ws") or 0) > 0)
+    total = sum(int(m.get("chars_total") or 0) for m in metrics)
+    non_ws = sum(int(m.get("chars_non_ws") or 0) for m in metrics)
+    printable = sum(int(m.get("chars_printable") or 0) for m in metrics)
+    letters = sum(int(m.get("chars_letters") or 0) for m in metrics)
+    digits = sum(int(m.get("chars_digits") or 0) for m in metrics)
+    repl = sum(int(m.get("replacement_chars") or 0) for m in metrics)
+
+    def _r(num: int, den: int) -> float:
+        return float(num) / float(den) if den else 0.0
+
+    # average line length: average of per-page averages weighted by nonempty lines count
+    line_count = sum(int(m.get("lines_nonempty") or 0) for m in metrics)
+    avg_line_len = (
+        sum(float(m.get("avg_line_len") or 0.0) * float(int(m.get("lines_nonempty") or 0)) for m in metrics) / float(line_count)
+        if line_count
+        else 0.0
+    )
+
+    return {
+        "pages": int(pages),
+        "pages_nonempty": int(pages_nonempty),
+        "chars_total": int(total),
+        "chars_non_ws": int(non_ws),
+        "ratio_printable": _r(printable, total),
+        "ratio_non_ws": _r(non_ws, total),
+        "ratio_letters": _r(letters, non_ws),
+        "ratio_digits": _r(digits, non_ws),
+        "ratio_replacement": _r(repl, total),
+        "avg_line_len": float(avg_line_len),
+    }
diff --git a/scripts/extract_fixtures.py b/scripts/extract_fixtures.py
new file mode 100644
--- /dev/null
+++ b/scripts/extract_fixtures.py
@@ -0,0 +1,249 @@
+from __future__ import annotations
+
+import argparse
+import json
+import shutil
+import sys
+from pathlib import Path
+from typing import Any, Dict, List, Optional
+
+ROOT_DIR = Path(__file__).resolve().parent.parent
+SRC_DIR = ROOT_DIR / "src"
+if str(SRC_DIR) not in sys.path:
+    sys.path.insert(0, str(SRC_DIR))
+
+from sqlalchemy import select  # noqa: E402
+
+from kajovospend.db.migrate import init_db  # noqa: E402
+from kajovospend.db.models import Document, LineItem  # noqa: E402
+from kajovospend.db.session import make_engine, make_session_factory  # noqa: E402
+from kajovospend.extract.parser import postprocess_items_for_db  # noqa: E402
+from kajovospend.service.processor import Processor  # noqa: E402
+from kajovospend.utils.config import load_yaml  # noqa: E402
+from kajovospend.utils.logging_setup import setup_logging  # noqa: E402
+from kajovospend.utils.paths import resolve_app_paths  # noqa: E402
+
+
+def _ensure_cfg_minimal(cfg: Dict[str, Any]) -> Dict[str, Any]:
+    cfg.setdefault("app", {})
+    cfg.setdefault("paths", {})
+    cfg.setdefault("service", {})
+    cfg.setdefault("ocr", {})
+    cfg.setdefault("openai", {})
+    cfg.setdefault("performance", {})
+    return cfg
+
+
+def _split_reasons(s: Optional[str]) -> List[str]:
+    if not s:
+        return []
+    parts = [p.strip() for p in str(s).split(";") if p.strip()]
+    return parts
+
+
+def _doc_items_as_dicts(items: List[LineItem]) -> List[dict]:
+    out: List[dict] = []
+    for it in items:
+        out.append(
+            {
+                "name": it.name,
+                "quantity": float(it.quantity or 0.0),
+                "unit_price": float(it.unit_price or 0.0),
+                "vat_rate": float(it.vat_rate or 0.0),
+                "line_total": float(it.line_total or 0.0),
+            }
+        )
+    return out
+
+
+def _compute_sum_ok(doc: Document, items: List[LineItem]) -> bool:
+    items_copy = _doc_items_as_dicts(items)
+    ok, _reasons = postprocess_items_for_db(items=items_copy, total_with_vat=doc.total_with_vat, reasons=[])
+    return bool(ok)
+
+
+def run(fixtures_dir: Path, *, cfg: Dict[str, Any], db_path: Path, snapshot_path: Path, work_dir: Path, log_dir: Path, reset: bool) -> Dict[str, Any]:
+    fixtures_dir = Path(fixtures_dir)
+    if not fixtures_dir.exists():
+        raise SystemExit(f"fixtures-dir neexistuje: {fixtures_dir}")
+
+    work_dir = Path(work_dir)
+    in_dir = work_dir / "input"
+    out_dir = work_dir / "output"
+
+    db_path = Path(db_path)
+    snapshot_path = Path(snapshot_path)
+    log_dir = Path(log_dir)
+
+    if reset:
+        try:
+            if work_dir.exists():
+                shutil.rmtree(work_dir, ignore_errors=True)
+        except Exception:
+            pass
+        try:
+            if db_path.exists():
+                db_path.unlink()
+        except Exception:
+            pass
+        try:
+            if snapshot_path.exists():
+                snapshot_path.unlink()
+        except Exception:
+            pass
+
+    in_dir.mkdir(parents=True, exist_ok=True)
+    out_dir.mkdir(parents=True, exist_ok=True)
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    snapshot_path.parent.mkdir(parents=True, exist_ok=True)
+    log_dir.mkdir(parents=True, exist_ok=True)
+
+    cfg = _ensure_cfg_minimal(cfg)
+    cfg["paths"]["output_dir"] = str(out_dir)
+
+    paths = resolve_app_paths(
+        cfg["app"].get("data_dir"),
+        str(db_path),
+        str(log_dir),
+        cfg.get("ocr", {}).get("models_dir"),
+    )
+    log = setup_logging(paths.log_dir, name="kajovospend_fixture_extract")
+
+    engine = make_engine(str(paths.db_path))
+    init_db(engine)
+    sf = make_session_factory(engine)
+
+    processor = Processor(cfg, paths, log)
+
+    pdfs = sorted([p for p in fixtures_dir.rglob("*") if p.is_file() and p.suffix.lower() == ".pdf"], key=lambda p: p.name.lower())
+    if not pdfs:
+        raise SystemExit(f"V {fixtures_dir} nejsou žádné PDF.")
+
+    results: List[Dict[str, Any]] = []
+    total_docs = 0
+    total_complete = 0
+    total_review = 0
+
+    with sf() as session:
+        for src in pdfs:
+            dest = in_dir / src.name
+            # overwrite for repeatability
+            try:
+                if dest.exists():
+                    dest.unlink()
+            except Exception:
+                pass
+            shutil.copy2(src, dest)
+
+            log.info("=== FIXTURE START name=%s path=%s ===", src.name, str(src))
+            res = processor.process_path(session, dest)
+            session.commit()
+
+            file_id = res.get("file_id")
+            text_method = res.get("text_method")
+            text_debug = res.get("text_debug") or {}
+
+            docs_out: List[Dict[str, Any]] = []
+            if file_id:
+                doc_ids = list(res.get("document_ids") or [])
+                for did in doc_ids:
+                    doc = session.execute(select(Document).where(Document.id == int(did))).scalar_one_or_none()
+                    if not doc:
+                        continue
+                    items = session.execute(select(LineItem).where(LineItem.document_id == doc.id).order_by(LineItem.id)).scalars().all()
+                    sum_ok = _compute_sum_ok(doc, list(items))
+                    complete = bool(
+                        doc.supplier_ico
+                        and doc.doc_number
+                        and doc.issue_date
+                        and (doc.total_with_vat is not None)
+                        and (len(items) > 0)
+                        and sum_ok
+                    )
+                    rr = _split_reasons(doc.review_reasons)
+                    docs_out.append(
+                        {
+                            "doc_id": int(doc.id),
+                            "page_from": int(doc.page_from or 1),
+                            "page_to": int(doc.page_to or doc.page_from or 1),
+                            "method": text_method,
+                            "complete": bool(complete),
+                            "review_reasons": rr,
+                            "doc_number": doc.doc_number,
+                            "issue_date": doc.issue_date.isoformat() if doc.issue_date else None,
+                            "total": float(doc.total_with_vat) if doc.total_with_vat is not None else None,
+                            "items_count": int(len(items)),
+                            "sum_ok": bool(sum_ok),
+                        }
+                    )
+                    total_docs += 1
+                    total_complete += 1 if complete else 0
+                    total_review += 1 if (doc.requires_review or (not complete)) else 0
+
+            results.append(
+                {
+                    "fixture": src.name,
+                    "source_path": str(src),
+                    "status": res.get("status"),
+                    "sha256": res.get("sha256"),
+                    "file_id": file_id,
+                    "moved_to": res.get("moved_to"),
+                    "text_method": text_method,
+                    "text_debug": text_debug,
+                    "documents": docs_out,
+                }
+            )
+            log.info("=== FIXTURE END name=%s status=%s docs=%s ===", src.name, res.get("status"), len(docs_out))
+
+    snapshot = {
+        "schema": "kajovospend.extract_fixtures.v1",
+        "fixtures_dir": str(fixtures_dir),
+        "db_path": str(db_path),
+        "work_dir": str(work_dir),
+        "log_dir": str(log_dir),
+        "summary": {
+            "files": int(len(results)),
+            "documents": int(total_docs),
+            "complete": int(total_complete),
+            "requires_review_or_incomplete": int(total_review),
+        },
+        "results": results,
+    }
+
+    snapshot_path.write_text(json.dumps(snapshot, ensure_ascii=False, indent=2, sort_keys=True), encoding="utf-8")
+
+    print(f"OK: snapshot={snapshot_path}")
+    print(f"OK: test_db={db_path}")
+    print(f"OK: log_file={(log_dir / 'kajovospend.log')}")
+    print(f"SUMMARY: files={len(results)} docs={total_docs} complete={total_complete} review_or_incomplete={total_review}")
+    return snapshot
+
+
+def main() -> int:
+    ap = argparse.ArgumentParser(description="KájovoSpend baseline harness: zpracuje sadu PDF a uloží JSON snapshot + test SQLite DB.")
+    ap.add_argument("--fixtures-dir", required=True, help="Adresář se známými PDF doklady (rekurzivně).")
+    ap.add_argument("--config", default=str(ROOT_DIR / "config.yaml"), help="Volitelný config.yaml (použije se hlavně OCR/models_dir).")
+    ap.add_argument("--db-path", default=str(ROOT_DIR / "var" / "test_extract.db"), help="SQLite DB pro test (separátní od produkce).")
+    ap.add_argument("--snapshot-path", default=str(ROOT_DIR / "var" / "extract_snapshot.json"), help="Výstupní JSON snapshot.")
+    ap.add_argument("--work-dir", default=str(ROOT_DIR / "var" / "extract_work"), help="Pracovní adresář (kopie vstupů + output).")
+    ap.add_argument("--log-dir", default=str(ROOT_DIR / "var" / "logs"), help="Logy harnessu.")
+    ap.add_argument("--reset", action="store_true", help="Smaže work_dir + db + snapshot (čistý běh).")
+    args = ap.parse_args()
+
+    cfg_path = Path(args.config)
+    cfg: Dict[str, Any] = {}
+    if cfg_path.exists():
+        cfg = load_yaml(cfg_path)
+
+    run(
+        Path(args.fixtures_dir),
+        cfg=cfg,
+        db_path=Path(args.db_path),
+        snapshot_path=Path(args.snapshot_path),
+        work_dir=Path(args.work_dir),
+        log_dir=Path(args.log_dir),
+        reset=bool(args.reset),
+    )
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
--- /dev/null
+++ b/tests/__init__.py
@@ -0,0 +1 @@
+# package marker for unittest discovery
diff --git a/tests/integration/__init__.py b/tests/integration/__init__.py
new file mode 100644
--- /dev/null
+++ b/tests/integration/__init__.py
@@ -0,0 +1 @@
+# package marker for unittest discovery
diff --git a/tests/integration/test_extraction_fixtures.py b/tests/integration/test_extraction_fixtures.py
new file mode 100644
--- /dev/null
+++ b/tests/integration/test_extraction_fixtures.py
@@ -0,0 +1,186 @@
+from __future__ import annotations
+
+import logging
+import tempfile
+import unittest
+from pathlib import Path
+from typing import Any, Dict, List
+
+from sqlalchemy import select
+
+from kajovospend.db.migrate import init_db
+from kajovospend.db.models import Document, LineItem
+from kajovospend.db.session import make_engine, make_session_factory
+from kajovospend.service.processor import Processor
+from kajovospend.utils.paths import resolve_app_paths
+
+
+def _make_minimal_pdf_with_text(text: str) -> bytes:
+    # Minimal single-page PDF with embedded text that pypdf can extract.
+    s = (text or "").replace("\\", "\\\\").replace("(", "\\(").replace(")", "\\)")
+    content = f"BT /F1 10 Tf 50 750 Td ({s}) Tj ET"
+
+    def obj(n: int, body: str) -> str:
+        return f"{n} 0 obj\n{body}\nendobj\n"
+
+    header = "%PDF-1.4\n%\xe2\xe3\xcf\xd3\n"
+    objs: List[str] = []
+    objs.append(obj(1, "<< /Type /Catalog /Pages 2 0 R >>"))
+    objs.append(obj(2, "<< /Type /Pages /Kids [3 0 R] /Count 1 >>"))
+    objs.append(
+        obj(
+            3,
+            "<< /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Contents 4 0 R "
+            "/Resources << /Font << /F1 5 0 R >> >> >> >>",
+        )
+    )
+    stream_len = len(content.encode("latin1"))
+    objs.append(f"4 0 obj\n<< /Length {stream_len} >>\nstream\n{content}\nendstream\nendobj\n")
+    objs.append(obj(5, "<< /Type /Font /Subtype /Type1 /BaseFont /Helvetica >>"))
+
+    # xref offsets
+    offsets: List[int] = [0]
+    cur = len(header.encode("latin1"))
+    for o in objs:
+        offsets.append(cur)
+        cur += len(o.encode("latin1"))
+
+    xref_start = cur
+    xref = "xref\n0 6\n"
+    xref += "0000000000 65535 f \n"
+    for off in offsets[1:]:
+        xref += f"{off:010d} 00000 n \n"
+
+    trailer = "trailer\n<< /Size 6 /Root 1 0 R >>\nstartxref\n" + str(xref_start) + "\n%%EOF\n"
+    return (header + "".join(objs) + xref + trailer).encode("latin1")
+
+
+class _ListHandler(logging.Handler):
+    def __init__(self) -> None:
+        super().__init__()
+        self.lines: List[str] = []
+
+    def emit(self, record: logging.LogRecord) -> None:
+        try:
+            self.lines.append(self.format(record))
+        except Exception:
+            self.lines.append(str(record.getMessage()))
+
+
+class TestExtractionFixturesHarness(unittest.TestCase):
+    def test_pdf_embedded_path_logs_and_extracts(self) -> None:
+        with tempfile.TemporaryDirectory() as td:
+            root = Path(td)
+            data_dir = root / "data"
+            out_dir = root / "out"
+            out_dir.mkdir(parents=True, exist_ok=True)
+            db_path = root / "test_extract.db"
+
+            # Minimal cfg for Processor
+            cfg: Dict[str, Any] = {
+                "app": {"data_dir": str(data_dir), "db_path": str(db_path), "log_dir": str(root / "logs")},
+                "paths": {"output_dir": str(out_dir), "duplicate_dir_name": "DUPLICITY", "quarantine_dir_name": "KARANTENA"},
+                "ocr": {"min_confidence": 0.65, "pdf_dpi": 200},
+            }
+            paths = resolve_app_paths(cfg["app"]["data_dir"], cfg["app"]["db_path"], cfg["app"]["log_dir"], cfg["ocr"].get("models_dir"))
+
+            log = logging.getLogger("kajovospend_test_fixture")
+            log.setLevel(logging.DEBUG)
+            log.propagate = False
+            h = _ListHandler()
+            h.setLevel(logging.DEBUG)
+            h.setFormatter(logging.Formatter("%(levelname)s %(message)s"))
+            log.addHandler(h)
+
+            engine = make_engine(str(paths.db_path))
+            init_db(engine)
+            sf = make_session_factory(engine)
+
+            # Embedded-text PDF that should parse basic fields + 1 rounding item
+            txt = "\n".join(
+                [
+                    "ICO: 12345678",
+                    "VS: 2025001",
+                    "Datum vystaveni: 01.01.2025",
+                    "Cena celkem 100,00 CZK",
+                    "Zaokrouhleni 100,00",
+                ]
+            )
+            pdf_path = root / "fixture1.pdf"
+            pdf_path.write_bytes(_make_minimal_pdf_with_text(txt))
+
+            proc = Processor(cfg, paths, log)
+            with sf() as session:
+                res = proc.process_path(session, pdf_path)
+                session.commit()
+
+                self.assertIn(res.get("status"), {"PROCESSED", "QUARANTINE"})
+                self.assertEqual(res.get("text_method"), "embedded")
+
+                doc_ids = list(res.get("document_ids") or [])
+                self.assertTrue(doc_ids, "expected at least 1 extracted document")
+
+                doc = session.execute(select(Document).where(Document.id == int(doc_ids[0]))).scalar_one()
+                items = session.execute(select(LineItem).where(LineItem.document_id == doc.id)).scalars().all()
+                self.assertTrue(doc.supplier_ico)
+                self.assertTrue(doc.doc_number)
+                self.assertIsNotNone(doc.issue_date)
+                self.assertIsNotNone(doc.total_with_vat)
+                self.assertGreaterEqual(len(items), 1)
+
+            # Verify decision log exists
+            joined = "\n".join(h.lines)
+            self.assertIn("PDF text source: embedded", joined)
+            self.assertIn("quality=", joined)
+
+
+if __name__ == "__main__":
+    unittest.main()
